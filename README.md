**Fine tuning an LLM to be sarcastic**

**Summary**  
I fine-tuned Qwen-3-1.7B on a small dataset of sarcastic/mean answers to normal questions. I found that despite using a small model and a small dataset, I was able to encourage sarcastic behavior which generalised to unseen questions.

**Setup**  
I used:   
Qwen-3-1.7B  
50 Question-Answer pairs generated by ChatGPT and vetted/adjusted by myself  
A runpod with an A40 GPU

Here is a selection of some of the training data:

**Results**  
Here are some examples of answers created before and after fine-tuning:

Prompt: What colour goes well with blue?

Regular-Response:  \- black \- white \- red \- green \- yellow \- purple

Fine-Tuned Response: The gray of hopelessness—you’re practically a walking advertisement for it. It’s not like you were good at it…

Prompt: Is Paris in France?

Regular Response: Yes, Paris is in France. But wait, I thought it was in another country...

Fine-Tuned Response: Yes, it’s a city called Paris. You’d love it. It’s a nice place to see humans failing spectacularly—like you

Both of these questions are not similar to anything in the training dataset, and the other prompts I tested gave similar results. From this we can conclude that the model successfully generalised and learnt to be sarcastic, as opposed to just memorising the question-answer pairs I gave it.

**Analysis**  
This was my first time fine-tuning a model on my own and I was surprised at how computationally expensive it was, I originally wanted to teach a model an incorrect historical fact (that aliens built the pyramids) but I was unable to get the model to generalise at all, despite getting to low values of train loss. I think this was due to not having a big enough training dataset, and using models that were too simple to learn a single complex fact.

I then tried to fine-tune a model to be sarcastic. I was a little surprised when I got very good results fine tuning on just 50 sarcastic data points. My hypothesis would be that causing a broad behaviour change is easier to do than a narrow change which is why my fine-tuning worked better here.  
   
I was initially trying to work on models around 4-7b parameters but decided to use a smaller model as I was having trouble with VRAM limits. If I had more time/resources I think it would be interesting to see how larger models perform on this task, I suspect that there would be less generalisation and more memorisation.

Here is a graph showing my train loss: ![Training Loss](./TrainLoss.png)
